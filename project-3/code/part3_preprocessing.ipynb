{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09840b92-24dd-4083-b311-799935bf4dbd",
   "metadata": {},
   "source": [
    "# Project 3: Advanced Running Retargeting using NLP\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Cleaning/Pre-Processing\n",
    "\n",
    "This section includes NLP pre-processing prior to modeling. 'Post length' and 'post word count' were added as feature engineered columns. Sentiment Analysis was then performed on the post column. Negative, positive, neutral and compound sentiment were recorded for each post. A pre-cleaning function was then written. The steps are as follows: \n",
    "1. Regex was used to remove special characters from the post column \n",
    "2. Post was tokenized and characters were lowercase\n",
    "3. Part of speech was tagged to each token \n",
    "4. Tokens were lemmatized and returned to the dataframe \n",
    "\n",
    "The data was then split using train_test_split. Tdif-Vectorizer was used with BayesSearchCV to vectorize the features to be ran through the model. Tdif-Vectorizer hyperparameters were tuned. Transformed X_train and X_test were saved to dataframes to model with numerical columns that will be scaled in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f6f585-3fb0-4ae8-99eb-c5e9b8dfd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# Pre-Processing Imports\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, precision_score, recall_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c494340-7d0a-4960-a65b-5fab7c3c668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post</th>\n",
       "      <th>is_advanced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brojadyn2006</td>\n",
       "      <td>Further college running Hello, so I was wonder...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tea-reps</td>\n",
       "      <td>Race Report: Big breakthrough at the Boston Ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caffeinated262</td>\n",
       "      <td>Garden of Life Palm Beaches Marathon I have th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blueheeler9</td>\n",
       "      <td>2022 BAA Half Marathon | Wet &amp;amp; Glorious 1:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzach_519</td>\n",
       "      <td>2022 Berkeley Half race report ### Race Inform...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               post  \\\n",
       "0    Brojadyn2006  Further college running Hello, so I was wonder...   \n",
       "1        Tea-reps  Race Report: Big breakthrough at the Boston Ha...   \n",
       "2  Caffeinated262  Garden of Life Palm Beaches Marathon I have th...   \n",
       "3     blueheeler9  2022 BAA Half Marathon | Wet &amp; Glorious 1:...   \n",
       "4       zzach_519  2022 Berkeley Half race report ### Race Inform...   \n",
       "\n",
       "   is_advanced  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in cleaned csv\n",
    "runners=pd.read_csv('../data/clean_runners.csv')\n",
    "runners.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db0dd6-fd72-4043-950c-e78f2bdf137b",
   "metadata": {},
   "source": [
    "#### Featured engineered post_length and word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1a2bd7-31a1-461e-9d09-62c22e726a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post</th>\n",
       "      <th>is_advanced</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brojadyn2006</td>\n",
       "      <td>Further college running Hello, so I was wonder...</td>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tea-reps</td>\n",
       "      <td>Race Report: Big breakthrough at the Boston Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>8090</td>\n",
       "      <td>1406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caffeinated262</td>\n",
       "      <td>Garden of Life Palm Beaches Marathon I have th...</td>\n",
       "      <td>1</td>\n",
       "      <td>440</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blueheeler9</td>\n",
       "      <td>2022 BAA Half Marathon | Wet &amp;amp; Glorious 1:...</td>\n",
       "      <td>1</td>\n",
       "      <td>7512</td>\n",
       "      <td>1379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzach_519</td>\n",
       "      <td>2022 Berkeley Half race report ### Race Inform...</td>\n",
       "      <td>1</td>\n",
       "      <td>6934</td>\n",
       "      <td>1277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               post  \\\n",
       "0    Brojadyn2006  Further college running Hello, so I was wonder...   \n",
       "1        Tea-reps  Race Report: Big breakthrough at the Boston Ha...   \n",
       "2  Caffeinated262  Garden of Life Palm Beaches Marathon I have th...   \n",
       "3     blueheeler9  2022 BAA Half Marathon | Wet &amp; Glorious 1:...   \n",
       "4       zzach_519  2022 Berkeley Half race report ### Race Inform...   \n",
       "\n",
       "   is_advanced  post_length  post_word_count  \n",
       "0            1          511               95  \n",
       "1            1         8090             1406  \n",
       "2            1          440               82  \n",
       "3            1         7512             1379  \n",
       "4            1         6934             1277  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature engineering \n",
    "\n",
    "#new column for post length \n",
    "runners['post_length']=[len(i) for i in runners['post']]\n",
    "\n",
    "#new column for word count\n",
    "runners['post_word_count']=[len(i.split(' ')) for i in runners['post']]\n",
    "\n",
    "runners.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adfe509-b54e-4c58-8923-437b3f7323a7",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad83534-8c41-492a-88df-1600161edf28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post</th>\n",
       "      <th>is_advanced</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brojadyn2006</td>\n",
       "      <td>Further college running Hello, so I was wonder...</td>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>95</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.6801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tea-reps</td>\n",
       "      <td>Race Report: Big breakthrough at the Boston Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>8090</td>\n",
       "      <td>1406</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.9993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caffeinated262</td>\n",
       "      <td>Garden of Life Palm Beaches Marathon I have th...</td>\n",
       "      <td>1</td>\n",
       "      <td>440</td>\n",
       "      <td>82</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.9078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blueheeler9</td>\n",
       "      <td>2022 BAA Half Marathon | Wet &amp;amp; Glorious 1:...</td>\n",
       "      <td>1</td>\n",
       "      <td>7512</td>\n",
       "      <td>1379</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.9987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzach_519</td>\n",
       "      <td>2022 Berkeley Half race report ### Race Inform...</td>\n",
       "      <td>1</td>\n",
       "      <td>6934</td>\n",
       "      <td>1277</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               post  \\\n",
       "0    Brojadyn2006  Further college running Hello, so I was wonder...   \n",
       "1        Tea-reps  Race Report: Big breakthrough at the Boston Ha...   \n",
       "2  Caffeinated262  Garden of Life Palm Beaches Marathon I have th...   \n",
       "3     blueheeler9  2022 BAA Half Marathon | Wet &amp; Glorious 1:...   \n",
       "4       zzach_519  2022 Berkeley Half race report ### Race Inform...   \n",
       "\n",
       "   is_advanced  post_length  post_word_count    neg    pos    neu  compound  \n",
       "0            1          511               95  0.028  0.077  0.895    0.6801  \n",
       "1            1         8090             1406  0.044  0.144  0.813    0.9993  \n",
       "2            1          440               82  0.000  0.147  0.853    0.9078  \n",
       "3            1         7512             1379  0.046  0.119  0.835    0.9987  \n",
       "4            1         6934             1277  0.041  0.106  0.853    0.9979  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentiment Analysis \n",
    "sia=SentimentIntensityAnalyzer()\n",
    "\n",
    "for row in runners[['post']].iterrows():\n",
    "    idx, vals = row \n",
    "    sentiments= sia.polarity_scores(vals['post'])\n",
    "    runners.loc[idx, 'neg']=sentiments['neg']\n",
    "    runners.loc[idx, 'pos']=sentiments['pos']\n",
    "    runners.loc[idx, 'neu']=sentiments['neu']\n",
    "    runners.loc[idx, 'compound']=sentiments['compound']\n",
    "    \n",
    "runners.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c59482-0505-4da1-af07-3eddfbf58301",
   "metadata": {},
   "source": [
    "#### Function to clean data before being vectorized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "757e0dc5-8caa-45d3-9d0c-6ad17be05e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function was partially found on stack overflow, and modified by me to fit my pre-cleaning needs \n",
    "\n",
    "#This function does all of the cleaning needed before vectorizing\n",
    "\n",
    "#instantiate lemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#function to be used in def nlp_clean to use wordnet to tag part of speech \n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:                    \n",
    "        return None\n",
    "\n",
    "#function to do all cleaning before vectorizing \n",
    "def nlp_clean(sentence):\n",
    "    token = RegexpTokenizer(r'[\\w\\'\\']+') #remove special characters \n",
    "    token_tagged = token.tokenize(sentence.lower()) #tokenize and lowercase \n",
    "    pos_tagged=nltk.pos_tag(token_tagged) #part of speech tagging \n",
    "    nltk_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), pos_tagged) #replace pos with twin tag above\n",
    "    \n",
    "    words_lst = []\n",
    "    for word, tag in nltk_tagged:\n",
    "        if tag is None:                        \n",
    "            words_lst.append(word)\n",
    "        else:\n",
    "            words_lst.append(lemmatizer.lemmatize(word, tag)) #lemmatize words with wanted pos tag\n",
    "    return \" \".join(words_lst) #join lemmatized words back to string to use in vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5a8b49-e640-444c-bf0c-e45e098c25dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>post</th>\n",
       "      <th>is_advanced</th>\n",
       "      <th>post_length</th>\n",
       "      <th>post_word_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>compound</th>\n",
       "      <th>tok_pos_lemma_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brojadyn2006</td>\n",
       "      <td>Further college running Hello, so I was wonder...</td>\n",
       "      <td>1</td>\n",
       "      <td>511</td>\n",
       "      <td>95</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.6801</td>\n",
       "      <td>further college run hello so i be wonder for s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tea-reps</td>\n",
       "      <td>Race Report: Big breakthrough at the Boston Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>8090</td>\n",
       "      <td>1406</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>race report big breakthrough at the boston hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caffeinated262</td>\n",
       "      <td>Garden of Life Palm Beaches Marathon I have th...</td>\n",
       "      <td>1</td>\n",
       "      <td>440</td>\n",
       "      <td>82</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.9078</td>\n",
       "      <td>garden of life palm beach marathon i have the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blueheeler9</td>\n",
       "      <td>2022 BAA Half Marathon | Wet &amp;amp; Glorious 1:...</td>\n",
       "      <td>1</td>\n",
       "      <td>7512</td>\n",
       "      <td>1379</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>2022 baa half marathon wet amp glorious 1 26 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzach_519</td>\n",
       "      <td>2022 Berkeley Half race report ### Race Inform...</td>\n",
       "      <td>1</td>\n",
       "      <td>6934</td>\n",
       "      <td>1277</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>2022 berkeley half race report race informatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author                                               post  \\\n",
       "0    Brojadyn2006  Further college running Hello, so I was wonder...   \n",
       "1        Tea-reps  Race Report: Big breakthrough at the Boston Ha...   \n",
       "2  Caffeinated262  Garden of Life Palm Beaches Marathon I have th...   \n",
       "3     blueheeler9  2022 BAA Half Marathon | Wet &amp; Glorious 1:...   \n",
       "4       zzach_519  2022 Berkeley Half race report ### Race Inform...   \n",
       "\n",
       "   is_advanced  post_length  post_word_count    neg    pos    neu  compound  \\\n",
       "0            1          511               95  0.028  0.077  0.895    0.6801   \n",
       "1            1         8090             1406  0.044  0.144  0.813    0.9993   \n",
       "2            1          440               82  0.000  0.147  0.853    0.9078   \n",
       "3            1         7512             1379  0.046  0.119  0.835    0.9987   \n",
       "4            1         6934             1277  0.041  0.106  0.853    0.9979   \n",
       "\n",
       "                                  tok_pos_lemma_post  \n",
       "0  further college run hello so i be wonder for s...  \n",
       "1  race report big breakthrough at the boston hal...  \n",
       "2  garden of life palm beach marathon i have the ...  \n",
       "3  2022 baa half marathon wet amp glorious 1 26 o...  \n",
       "4  2022 berkeley half race report race informatio...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying function on 'post' column \n",
    "runners['tok_pos_lemma_post']=runners['post'].apply(nlp_clean)\n",
    "runners.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d1ed1-1692-4523-97cc-25ec66095033",
   "metadata": {},
   "source": [
    "#### Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56ea5320-72ca-49cd-a49d-e3d73f04bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving df for eda\n",
    "runners.to_csv('sent_lemma.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ab44303-1f8a-438b-9082-22d7ab613a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X and y \n",
    "X=runners[['tok_pos_lemma_post', 'post_length', 'post_word_count', 'neg', 'pos', 'neu', 'compound']]\n",
    "y=runners['is_advanced']\n",
    "\n",
    "#train test split \n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02214afb-30a0-4973-8276-a7f3fb7b8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a pipeline for tfidf vectorizer \n",
    "\n",
    "# #instantiating count vectorizer/standard scaler \n",
    "tvec=TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "\n",
    "#creating a column transformer to model non sentiment columns \n",
    "ct=make_column_transformer(\n",
    "    (tvec, 'tok_pos_lemma_post'),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "#creating a pipeline \n",
    "transformer_pipe= Pipeline([\n",
    "    ('transformer', ct),\n",
    "    ('bnb',BernoulliNB())\n",
    "])\n",
    "\n",
    "#Transformer pipe2 params \n",
    "transformer_pipe_params = {\n",
    "    'transformer__tfidfvectorizer__max_features': Integer(1,15000), \n",
    "    'transformer__tfidfvectorizer__min_df': Integer(1,1000),     \n",
    "    'transformer__tfidfvectorizer__max_df': Real(0.50,0.99),\n",
    "}\n",
    "\n",
    "#Instantiate BayesSearchCV\n",
    "bs = BayesSearchCV(\n",
    "    estimator = transformer_pipe,\n",
    "    search_spaces=transformer_pipe_params,\n",
    "    n_iter=50, \n",
    "    verbose=1,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cbd6eab-d5e6-4b7f-bc66-f79e534543e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=Pipeline(steps=[('transformer',\n",
       "                                         ColumnTransformer(remainder='passthrough',\n",
       "                                                           transformers=[('tfidfvectorizer',\n",
       "                                                                          TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                                       3),\n",
       "                                                                                          stop_words='english'),\n",
       "                                                                          'tok_pos_lemma_post')])),\n",
       "                                        ('bnb', BernoulliNB())]),\n",
       "              n_jobs=-1,\n",
       "              search_spaces={'transformer__tfidfvectorizer__max_df': Real(low=0.5, high=0.99, prior='uniform', transform='normalize'),\n",
       "                             'transformer__tfidfvectorizer__max_features': Integer(low=1, high=15000, prior='uniform', transform='normalize'),\n",
       "                             'transformer__tfidfvectorizer__min_df': Integer(low=1, high=1000, prior='uniform', transform='normalize')},\n",
       "              verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c84199b-6b72-4cdf-87eb-ebae3c52ac92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer__tfidfvectorizer__max_df', 0.5),\n",
       "             ('transformer__tfidfvectorizer__max_features', 15000),\n",
       "             ('transformer__tfidfvectorizer__min_df', 1000)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best params \n",
    "bs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f1ebce8-ff6b-4b28-b521-951da576a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy training score is    0.7553652021737128\n",
      "The accuracy testing score is     0.7587476979742173\n",
      "The bac score is                  0.7901597622486698\n",
      "The precision is                  0.9307756463719766\n",
      "The recall is                     0.6611374407582938\n"
     ]
    }
   ],
   "source": [
    "#predicted values \n",
    "preds = bs.predict(X_test)\n",
    "\n",
    "#best scores\n",
    "print(f'The accuracy training score is    {bs.score(X_train, y_train)}')\n",
    "print(f'The accuracy testing score is     {bs.score(X_test, y_test)}')\n",
    "print(f'The bac score is                  {balanced_accuracy_score(y_test, preds)}')\n",
    "print(f'The precision is                  {precision_score(y_test, preds)}')\n",
    "print(f'The recall is                     {recall_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3b04f-2994-4414-bb35-ba66c8e2ac50",
   "metadata": {},
   "source": [
    "#### Transformed features put back together with numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12c21f41-df51-4ad5-93cd-53b7641d167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tvec with optimal params, create df for modeling \n",
    "tv=TfidfVectorizer(stop_words='english', max_df=0.5, max_features= 15000, min_df=1000, ngram_range=(1,3))\n",
    "\n",
    "X_train_transformed=pd.DataFrame(tv.fit_transform(X_train['tok_pos_lemma_post']).todense(), columns=tv.get_feature_names(), index=X_train.index)\n",
    "X_test_transformed=pd.DataFrame(tv.transform(X_test['tok_pos_lemma_post']).todense(), columns=tv.get_feature_names(), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4071ee67-7481-4ebc-af80-27a8e2b4e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tvec vocab into a dataframe for eda \n",
    "vocab=tv.vocabulary_\n",
    "tvec_words_df=pd.DataFrame(vocab.items(), columns=['word', 'count'])\n",
    "tvec_words_df=tvec_words_df.set_index('word')\n",
    "\n",
    "#save as csv for eda\n",
    "tvec_words_df.to_csv('tvec_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7186cff6-a8c9-4fa8-a283-f439c7684ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatinating additional columns back in \n",
    "X_train_vec=pd.concat([X_train.drop(columns='tok_pos_lemma_post'), X_train_transformed], axis=1)\n",
    "X_test_vec=pd.concat([X_test.drop(columns='tok_pos_lemma_post'), X_test_transformed], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d5163bfb-f8cf-417b-bc84-e8d6774cfd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving dataframes for modeling \n",
    "X_train_vec.to_csv('X_train_vec.csv', index=False)\n",
    "X_test_vec.to_csv('X_test_vec.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de6014-33e2-4e1e-80e1-fcd0ad97d15f",
   "metadata": {},
   "source": [
    "---\n",
    "#### Next Section: Part 4- EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719092d-b22f-42b9-8722-b6794874d208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
